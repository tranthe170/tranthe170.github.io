[
{
	"uri": "/",
	"title": "Welcome to the AWS Workshop",
	"tags": [],
	"description": "",
	"content": "Glue ETL with Apache Spark This workshop will guide you through the essentials of AWS data analysis tools. Get ready to explore Amazon S3, AWS Glue, Amazon Athena, and Amazon QuickSight, and discover how they can simplify your data analysis tasks. What Will You Learn? In this workshop, we’ll teach you how to use some of the key AWS tools to simplify data analysis:\nAmazon S3: Learn how to store your data safely and access it whenever you need. AWS Glue: Discover how to clean and organize your data automatically so it’s ready for analysis. Amazon Athena: See how to run quick and easy searches on your data using SQL, without needing to set up any servers. Amazon QuickSight: Learn how to create simple and clear charts and dashboards to help you understand important trends in your data. Objectives In this part of the workshop, we’ll focus on:\nQuerying the Data: We’ll use a data lake to answer common questions, like identifying the top universities in the world, ranking universities by region, comparing private and public universities, and counting the number of universities by region and country.\nHandling Data Quality Issues: We’ll use SQL to clean up the data and make it ready for analysis.\nSimplifying Queries: We’ll look at techniques to make querying data easier.\nVisualizing Data: Finally, we’ll use Amazon QuickSight, a Business Intelligence tool, to create visual representations of the data.\nTable of Contents Introduction to Glue ETL Service Exploratory Data Analysis Preparation Steps 3.1. Prepare S3 Bucket 3.2. Prepare AWS Glue 3.3. Query with Athena 3.4. Visualizing Data with Amazon QuickSight Glue ETL Job Clean Up Resources "
},
{
	"uri": "/1-introduce/",
	"title": "Introduction to Glue ETL Service",
	"tags": [],
	"description": "",
	"content": "ETL ETL stands for Extract, Transform, and Load. It\u0026rsquo;s a process used to move data from one place to another while preparing it for analysis:\nExtract: Get data from various sources. Transform: Clean and change the data to make it ready for analysis. This can include combining data from different places, fixing data errors, and converting data into the right format. Load: Put the clean data into a final storage place, like a data lake or a data warehouse. AWS Glue provides a fully managed ETL service that runs your data transformation jobs on a serverless Apache Spark environment. This means you don’t have to worry about setting up and managing servers; AWS Glue handles it for you.\nDataFrame, Spark DataFrame, and Glue DynamicFrame DataFrame: In Python and R, a DataFrame is a way to store data in memory, making it easy to work with. You can load data from files or databases and quickly analyze it. DataFrames are great for small to medium datasets that fit on one machine.\nSpark DataFrame: Spark DataFrames are like regular DataFrames but are designed to work with large datasets across multiple machines. Spark handles the heavy lifting, running your code where the data is stored, making it faster and more efficient for big data.\nGlue DynamicFrame: AWS Glue introduces DynamicFrames, which are similar to Spark DataFrames but are specially designed for ETL jobs in Glue. They have extra features like handling data with mixed types (e.g., a column with both numbers and text) and integrating easily with other AWS services.\nGlue ETL Bookmark AWS Glue can keep track of the data it has already processed using something called a job bookmark. This is useful when you want to process only new data and avoid reprocessing old data. Glue bookmarks work by tracking changes in your data, whether it’s files in S3 or rows in a database.\nGlue Job Types AWS Glue offers several types of jobs:\nSpark Jobs: These run in a serverless Spark environment and are usually used for batch processing, where you process data in chunks.\nStreaming ETL Jobs: These are for processing data continuously from streaming sources like Kinesis Streams or Kafka, delivering new data within minutes.\nPython Shell Jobs: These run Python scripts that don’t need Spark. They are simpler and quicker for tasks that don’t require heavy data processing.\nRay.io Jobs: This is a newer job type in Glue, designed for machine learning workflows using the Ray.io framework.\nSummary Python DataFrames: Best for small to medium datasets on a single machine. Spark DataFrames: Ideal for large datasets, using multiple machines for faster processing. Glue DynamicFrames: Optimized for ETL tasks in AWS Glue, with special features for data transformation. "
},
{
	"uri": "/2-eda/",
	"title": "Exploratory Data Analysis",
	"tags": [],
	"description": "",
	"content": "Dataset Overview URL for the dataset: https://www.kaggle.com/datasets/padhmam/qs-world-university-rankings-2017-2022\nThis dataset comes from Kaggle and ranks universities based on several factors:\nUniversity Name: The name of the university. Year of Ranking: The specific year the ranking applies to. Rank Assigned: The rank given to the university for that year. Raw Score: A score that contributes to the rank. Country: The country where the university is located. Region: The broader geographical region. Public or Private: Whether the university is publicly or privately funded. University Size: The size of the university, often in terms of student population. Why This Dataset? I chose this dataset because it’s easy to understand, useful, and includes a mix of different types of data. Plus, it has some data quality issues, which makes it a great example for learning how to clean up data. By working with this dataset, you\u0026rsquo;ll get hands-on experience in identifying and resolving common data issues, which is a crucial skill in data analysis.\n"
},
{
	"uri": "/3-preparation/",
	"title": "Preparation Steps",
	"tags": [],
	"description": "",
	"content": "In this section, we will outline the key steps you\u0026rsquo;ll follow in the next pages of this workshop. These steps are crucial for setting up the environment and tools you’ll need for data analysis with AWS.\n1. Prepare S3 Bucket Learn how to create and configure an Amazon S3 bucket to store your data securely.\nGo to Preparation\n2. Prepare AWS Glue Discover how to set up AWS Glue to transform and prepare your data for analysis.\nGo to Preparation\n3. Query with Athena Understand how to use Amazon Athena to query your data stored in S3 using SQL.\nGo to Preparation\n4. Visualizing Data with Amazon QuickSight Explore how to visualize your data and create dashboards using Amazon QuickSight.\nGo to Preparation\n"
},
{
	"uri": "/3-preparation/3.1-s3-bucket/",
	"title": "Prepare S3 Bucket",
	"tags": [],
	"description": "",
	"content": "Access the S3 Management Console Log in to the AWS Management Console:\nUse your credentials to log in to the AWS Management Console. Search for and select \u0026ldquo;S3\u0026rdquo;:\nIn the AWS Management Console, use the search bar at the top to search for \u0026ldquo;S3.\u0026rdquo; Click on \u0026ldquo;S3\u0026rdquo; in the search results to open the S3 service. Create a New S3 Bucket Click on \u0026ldquo;Create bucket\u0026rdquo;:\nOn the S3 dashboard, click the \u0026ldquo;Create bucket\u0026rdquo; button. Bucket name:\nEnter a unique name for your bucket (e.g., my-data-bucket). Remember that the bucket name must be globally unique. Block Public Access settings:\nKeep the default settings to block all public access to your bucket. This ensures that your data remains secure. Click \u0026ldquo;Create bucket\u0026rdquo;:\nOnce you have entered all the required information, click the \u0026ldquo;Create bucket\u0026rdquo; button. Your S3 bucket is now ready, and you can use it to store the data that will be processed in later stages of the workshop.\nOrganize Your S3 Bucket and Upload Data Create a folder \u0026ldquo;university_ranking\u0026rdquo;:\nClick on your bucket name to enter the bucket, then click \u0026ldquo;Create folder\u0026rdquo; and name it university_ranking. Create a subfolder \u0026ldquo;raw\u0026rdquo; and upload data:\nInside the university_ranking folder, create another folder named raw. Enter the raw folder and click \u0026ldquo;Upload.\u0026rdquo; Upload your data:\nDrag and drop your data files into the upload area, then click \u0026ldquo;Upload\u0026rdquo; to store the data in the raw folder. "
},
{
	"uri": "/3-preparation/3.2-aws-glue/",
	"title": "Prepare AWS Glue",
	"tags": [],
	"description": "",
	"content": "Create a Database in AWS Glue Access the AWS Glue Console Search for and select \u0026ldquo;Glue\u0026rdquo;: In the AWS Management Console, use the search bar at the top to search for \u0026ldquo;Glue.\u0026rdquo; Click on \u0026ldquo;Glue\u0026rdquo; in the search results to open the AWS Glue service. Create a Database In the left-hand menu, click on \u0026ldquo;Databases\u0026rdquo;:\nUnder the \u0026ldquo;Data catalog\u0026rdquo; section, click on \u0026ldquo;Databases.\u0026rdquo; Click on \u0026ldquo;Add database\u0026rdquo;:\nClick the \u0026ldquo;Add database\u0026rdquo; button to start creating a new database. Database name:\nEnter a name for your database (e.g., my_glue_database) and click \u0026ldquo;Create.\u0026rdquo; Create AWS Glue Roles Create AWSGlueServiceRoleDefault Role Go to the IAM Console:\nUse the AWS Management Console to navigate to the IAM service. Click on \u0026ldquo;Roles\u0026rdquo;:\nIn the left-hand menu, click on \u0026ldquo;Roles.\u0026rdquo; Click \u0026ldquo;Create role\u0026rdquo;:\nClick the \u0026ldquo;Create role\u0026rdquo; button to start the process. Select type of trusted entity and choose use case \u0026ldquo;Glue\u0026rdquo;:\nChoose \u0026ldquo;AWS service.\u0026rdquo; Then, select \u0026ldquo;Glue\u0026rdquo; as the use case. Attach policies:\nClick \u0026ldquo;Next\u0026rdquo; to proceed to the permissions step. Search for and select \u0026ldquo;AWSGlueServiceRole.\u0026rdquo; Role name:\nClick \u0026ldquo;Next.\u0026rdquo; Enter AWSGlueServiceRoleDefault as the role name. Click \u0026ldquo;Create role\u0026rdquo;:\nClick the \u0026ldquo;Create role\u0026rdquo; button to finalize. Create AWSGlueServiceNotebookRoleDefault Role To properly run AWS Glue notebooks on a Spark cluster, you need to create a specific IAM role (AWSGlueServiceNotebookRoleDefault) with appropriate permissions. This role grants permissions to the notebook, but the actual data processing is handled by the Spark cluster. Therefore, you must allow the notebook to pass its permissions to the Spark cluster using the PassRole permission.\nCreate another role following the same steps:\nRepeat the process outlined above to create a new role, naming it AWSGlueServiceNotebookRoleDefault. Add PassRole permission:\nSelect the AWSGlueServiceNotebookRoleDefault role you just created and click \u0026ldquo;Create inline policy.\u0026rdquo; Switch to the JSON view and paste the following code: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;iam:PassRole\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:iam::YOUR_ACCOUNT_ID:role/AWSGlueServiceNotebookRoleDefault\u0026#34; } ] } Replace YOUR_ACCOUNT_ID with your own AWS Account ID. Name and create the policy:\nEnter PassRolePermission as the policy name. Finalize the role:\nClick \u0026ldquo;Create role\u0026rdquo; to complete the process. Create a Crawler in AWS Glue Access the Crawlers Section In the AWS Glue console, click on \u0026ldquo;Crawlers\u0026rdquo;:\nIn the left-hand menu, select \u0026ldquo;Crawlers.\u0026rdquo; Click \u0026ldquo;Add crawler\u0026rdquo;:\nClick the \u0026ldquo;Add crawler\u0026rdquo; button to begin the setup. Configure the Crawler Name:\nEnter a name for your crawler (e.g., my_crawler). Data source:\nSelect \u0026ldquo;Add a data source.\u0026rdquo; Include path:\nChoose \u0026ldquo;S3\u0026rdquo; as the data source. Enter the S3 bucket path where your data is stored (e.g., s3://my-data-bucket/). IAM Role:\nSelect the AWSGlueServiceRoleDefault role created earlier. Output:\nChoose the database you created earlier (e.g., my_glue_database). Enter university_ranking_ for the prefix. Set the Crawler schedule to \u0026ldquo;On demand.\u0026rdquo; Review and click \u0026ldquo;Finish\u0026rdquo;:\nReview your configuration and click \u0026ldquo;Finish\u0026rdquo; to create the crawler. Run the Crawler Select the crawler:\nIn the Crawlers section, select the crawler you just created. Click \u0026ldquo;Run crawler\u0026rdquo;:\nClick the \u0026ldquo;Run crawler\u0026rdquo; button to start the process. Wait for the crawler to finish running:\nThe crawler will catalog the data in your S3 bucket. Create a Glue Notebook Access the Notebooks Section In the AWS Glue console, click on \u0026ldquo;Notebooks\u0026rdquo;:\nIn the left-hand menu, click on \u0026ldquo;Notebooks.\u0026rdquo; Click \u0026ldquo;Notebook\u0026rdquo;:\nClick the \u0026ldquo;Notebook\u0026rdquo; button to start setting up a new notebook. Configure the Notebook Configure the notebook:\nChoose \u0026ldquo;Start fresh,\u0026rdquo; select the AWSGlueServiceNotebookRoleDefault role, and click \u0026ldquo;Create notebook.\u0026rdquo; Rename the notebook:\nEnter \u0026ldquo;university_ranking_nb\u0026rdquo; as the notebook name. Click \u0026ldquo;Save\u0026rdquo;:\nClick the \u0026ldquo;Save\u0026rdquo; button to finalize. You can now write and execute Spark and Python code to process and analyze your data in the Glue notebook.\n"
},
{
	"uri": "/3-preparation/3.3-athena/",
	"title": "Query with Athena",
	"tags": [],
	"description": "",
	"content": "Access the Athena Console Search for and select \u0026ldquo;Athena\u0026rdquo;: In the AWS Management Console, use the search bar at the top to search for \u0026ldquo;Athena.\u0026rdquo; Click on \u0026ldquo;Athena\u0026rdquo; in the search results to open the Athena service. Configure Athena Set Up Query Result Location:\nEnsure you have set up a query result location in S3. This is where Athena will store the results of your queries. If you haven’t set this up yet, you’ll be prompted to do so when you try to run your first query. Select or create an S3 bucket to store your query results. Select Database:\nIn the Query Editor, select the Glue database you created earlier (e.g., my_glue_database). This will allow you to run queries against the data cataloged by AWS Glue. Run Queries Write SQL Queries:\nUse the SQL query editor to write queries that analyze the data stored in your S3 bucket and cataloged by Glue. Example Query:\nRun a simple query to check the data: SELECT * FROM my_table LIMIT 10; Replace my_table with the actual table name in your Glue database.\nRun and Review the Query:\nClick \u0026ldquo;Run query\u0026rdquo; to execute your SQL statement. Review the results in the Query Editor. The results will also be saved in the S3 bucket you configured earlier. This step allows you to analyze the data cataloged by AWS Glue using Athena, enabling you to run SQL queries directly on your data stored in S3.\n"
},
{
	"uri": "/3-preparation/3.4-quicksight/",
	"title": "Visualizing Data with Amazon QuickSight",
	"tags": [],
	"description": "",
	"content": "Visualizing Data with Amazon QuickSight Access the QuickSight Console Search for and select \u0026ldquo;QuickSight\u0026rdquo;: In the AWS Management Console, use the search bar at the top to search for \u0026ldquo;QuickSight.\u0026rdquo; Click on \u0026ldquo;QuickSight\u0026rdquo; in the search results to open the QuickSight service. Set Up QuickSight Sign up for QuickSight:\nIf this is your first time using QuickSight, you\u0026rsquo;ll need to sign up for the service. Follow the on-screen instructions to complete the setup. Add Data Sources:\nOnce you\u0026rsquo;re in the QuickSight console, click on \u0026ldquo;Datasets\u0026rdquo; from the left-hand menu. Add a new data source by selecting \u0026ldquo;New dataset.\u0026rdquo; Choose Athena as the Data Source:\nSelect \u0026ldquo;Athena\u0026rdquo; from the list of available data sources. Connect to the Glue database you created earlier by selecting it from the list of databases. "
},
{
	"uri": "/4-etl-pipelines/",
	"title": "Glue ETL Job",
	"tags": [],
	"description": "",
	"content": "Glue ETL Job: Handling Data Issues In this section, we\u0026rsquo;ll demonstrate how to process a dataset using an AWS Glue ETL job, addressing common data quality issues and transforming the data into a clean, analysis-ready format.\nDataset Issues The university rankings dataset has several issues that need to be addressed before it can be used effectively for analysis:\nMissing or Incomplete Data: Some records have missing or incomplete fields such as the year, rank, score, or student-faculty ratio. Inconsistent Formatting: Data fields like international_students and faculty_count might contain non-numeric characters (e.g., commas or dots) that need to be cleaned. Inconsistent Data Types: Fields like rank_display may contain ranges (e.g., \u0026ldquo;101-150\u0026rdquo;), which need to be split and converted into integers for analysis. ETL Script Overview The following script demonstrates how to address these issues using AWS Glue. The script reads the data from the Glue Data Catalog, cleans and transforms it, and then writes the cleaned data back to S3.\nimport sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job from awsglue.dynamicframe import DynamicFrame sc = SparkContext.getOrCreate() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext) # Read data from Glue catalog university_ranking_source = glueContext.create_dynamic_frame.from_catalog( database=\u0026#34;demo_db\u0026#34;, table_name=\u0026#34;university_ranking_csv\u0026#34;, transformation_ctx=\u0026#34;UniversityRanking_Source\u0026#34;, ) # Print the schema to understand the structure of the dataset university_ranking_source.printSchema() # Convert DynamicFrame to DataFrame for SQL operations rankings_df = university_ranking_source.toDF() rankings_df.show(5) # Register the DataFrame as a temporary SQL view for transformation rankings_df.createOrReplaceTempView(\u0026#34;university_ranking\u0026#34;) # Perform data cleanup and transformation clean_rankings_df = spark.sql(\u0026#34;\u0026#34;\u0026#34; SELECT university, COALESCE(INT(year), 9999) AS year, -- Handle missing years rank_display, COALESCE(INT(SPLIT(rank_display, \u0026#39;-\u0026#39;)[0]), 9999) AS n_rank, -- Split and convert rank ranges to integers COALESCE(FLOAT(score), -1) AS score, -- Handle missing scores country, city, region, type, research_output, COALESCE(FLOAT(student_faculty_ratio), -1) AS student_faculty_ratio, -- Handle missing student-faculty ratios COALESCE(INT(REGEXP_REPLACE(international_students, \u0026#39;[.,]\u0026#39;, \u0026#39;\u0026#39;)), -1) AS international_students, -- Clean and convert international students size, COALESCE(INT(REGEXP_REPLACE(faculty_count, \u0026#39;[.,]\u0026#39;, \u0026#39;\u0026#39;)), -1) AS faculty_count -- Clean and convert faculty count FROM university_ranking \u0026#34;\u0026#34;\u0026#34;) clean_rankings_df.show(5) # Convert the cleaned DataFrame back to DynamicFrame clean_dynamic_frame = DynamicFrame.fromDF(clean_rankings_df, glueContext, \u0026#34;university_ranking_clean\u0026#34;) # Write the cleaned data back to S3 s3output = glueContext.getSink( path=\u0026#34;s3://aws-glue-simple-demo/university_ranking/clean\u0026#34;, connection_type=\u0026#34;s3\u0026#34;, updateBehavior=\u0026#34;UPDATE_IN_DATABASE\u0026#34;, partitionKeys=[], compression=\u0026#34;snappy\u0026#34;, enableUpdateCatalog=True, transformation_ctx=\u0026#34;s3output\u0026#34;, ) s3output.setCatalogInfo( catalogDatabase=\u0026#34;demo_db\u0026#34;, catalogTableName=\u0026#34;university_ranking_clean\u0026#34; ) s3output.setFormat(\u0026#34;glueparquet\u0026#34;) s3output.writeFrame(clean_dynamic_frame) # Commit the job to save state and bookmarks job.commit() Explanation of the ETL Process Reading Data:\nThe script begins by reading the dataset from the Glue Data Catalog into a DynamicFrame. It prints the schema to help understand the structure of the dataset. Transformation:\nThe dataset is converted to a DataFrame to leverage SQL queries for data transformation. Key transformations include: Handling Missing Values: Missing years and scores are replaced with default values (9999 for years, -1 for scores). Data Type Consistency: Fields like rank_display are split and converted into integers. Cleaning Data: Fields such as international_students and faculty_count are cleaned by removing non-numeric characters before converting them to integers. Writing Data:\nThe cleaned and transformed data is converted back into a DynamicFrame. It is then written to an S3 bucket in Parquet format, making it ready for further analysis. Conclusion This ETL job demonstrates how to address common data quality issues in a dataset using AWS Glue. By handling missing values, ensuring data type consistency, and cleaning the data, we can prepare the dataset for accurate and efficient analysis.\n"
},
{
	"uri": "/5-clean-up/",
	"title": "Clean Up Resources",
	"tags": [],
	"description": "",
	"content": "After completing the workshop, it\u0026rsquo;s important to clean up the resources you\u0026rsquo;ve created to avoid unnecessary charges. Follow the steps below to safely delete the AWS resources used in this workshop.\n1. Delete S3 Buckets Access the S3 Console:\nGo to the AWS Management Console and navigate to the S3 service. Select and Delete Buckets:\nFind the S3 bucket(s) you created for this workshop (e.g., my-data-bucket). Select the bucket(s) and click \u0026ldquo;Delete.\u0026rdquo; Confirm the deletion by typing the bucket name. 2. Delete Glue Databases and Crawlers Access the Glue Console:\nGo to the AWS Glue console. Delete Databases:\nNavigate to the \u0026ldquo;Databases\u0026rdquo; section under the \u0026ldquo;Data Catalog.\u0026rdquo; Select the database(s) you created (e.g., my_glue_database) and delete them. Delete Crawlers:\nGo to the \u0026ldquo;Crawlers\u0026rdquo; section. Select the crawler(s) you created and delete them. 3. Delete Athena Query Results Access the S3 Console:\nAthena stores query results in an S3 bucket. Go to the S3 service. Delete Query Results:\nNavigate to the S3 bucket where Athena query results are stored. Delete the query results files. 4. Delete QuickSight Resources Access the QuickSight Console:\nGo to the AWS QuickSight console. Delete Data Sources and Dashboards:\nNavigate to \u0026ldquo;Manage data\u0026rdquo; and delete the data sources you created. Delete any dashboards or visualizations you created during the workshop. 5. Delete IAM Roles Access the IAM Console:\nGo to the AWS IAM console. Delete the Roles:\nNavigate to the \u0026ldquo;Roles\u0026rdquo; section. Delete the roles you created for Glue (e.g., AWSGlueServiceRoleDefault, AWSGlueServiceNotebookRoleDefault). By following these steps, you\u0026rsquo;ll ensure that all the resources created during this workshop are properly removed, helping you avoid unnecessary charges.\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]